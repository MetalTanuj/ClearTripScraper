{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "anomolous Data encountered\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from random import randint\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "url='https://me.cleartrip.com/hotels/united-states/miami?page=27'\n",
    "MAX_PAGE_NUM=38\n",
    "\n",
    "#initialising the webdriver    \n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "\n",
    "l=range(0,MAX_PAGE_NUM)\n",
    "data={}\n",
    "List=[]\n",
    "\n",
    "\n",
    "write_file=open(\"/Users/tanujmathur/data_file.json\", \"w\")\n",
    "                   \n",
    "#iteration for all the webpages\n",
    "for i in l: #start i from 1\n",
    "    i=i+1\n",
    "    driver.get(url)\n",
    "    \n",
    "    ids=driver.find_elements_by_xpath('//*[@id=\"hotelsList\"]//*[@class=\"hotels-name\"]//*[@href]')\n",
    "    \n",
    "    for link in ids:\n",
    "        j=j+1\n",
    "        print(j)\n",
    "        \n",
    "        #retrieving data from the webpages\n",
    "        source = requests.get(link.get_attribute(\"href\"))\n",
    "        page = source.text\n",
    "        soup = BeautifulSoup(page,\"lxml\")\n",
    "        \n",
    "        #using try block to avoid failure due to poor data\n",
    "        try:\n",
    "            header=soup.find('div',class_='container hotelDetailsHeaderContainer clearFix')\n",
    "        \n",
    "            name=header.h1.text\n",
    "            data[\"hotel_name\"]=name.strip('\\n').strip('\\t')\n",
    "        \n",
    "            data['hotel_url']=link.get_attribute(\"href\")\n",
    "        \n",
    "            rat=header.li.text\n",
    "            data[\"hotel_rating\"]=rat.strip('\\n').strip('\\t')\n",
    "        \n",
    "            \n",
    "            loc=soup.find('div',class_='highlightsWrapper clearFix')\n",
    "            location=loc.h5.text\n",
    "            data[\"hotel-location\"]=location.strip('\\n').strip('\\t')\n",
    "        \n",
    "            about=soup.find('div',class_=\"amenitiesCategory\").text\n",
    "            data[\"hotel_info\"]=about.strip('\\n').strip('\\t').replace('\\n', ' ').replace('\\\"', ' ')\n",
    "            qfacts=soup.find('div',class_='row amenitiesContainer content').find(class_='hInfo row')\n",
    "            chckin=qfacts.find_all('li')[0].find('span').text\n",
    "            data['check-in']=chckin.strip('\\n').strip('\\t')\n",
    "        \n",
    "            chckout=qfacts.find_all('li')[1].find('span').text\n",
    "            data['check-out']=chckout.strip('\\n').strip('\\t')\n",
    "        \n",
    "            rooms=qfacts.find_all('li')[2].find('span').text\n",
    "            data['rooms']=rooms.strip('\\n').strip('\\t')\n",
    "        \n",
    "            floors=qfacts.find_all('li')[3].find('span').text\n",
    "            data['floors']=floors.strip('\\n').strip('\\t')  \n",
    "        \n",
    "            \n",
    "            busSer=soup.find('div',class_='row amenitiesContainer content').find_all(class_='amenitiesCategory')\n",
    "            for a in range(len(busSer)): \n",
    "                if busSer[a].find('strong').text=='Business Services':\n",
    "                    l1=busSer[a].find(class_='checkList row').text\n",
    "                    data['Business-Services']=l1.split()\n",
    "                if busSer[a].find('strong').text=='Basics':\n",
    "                    l2=busSer[a].find('ul', class_='checkList row').text\n",
    "                    data['Basics']=l2.split()\n",
    "                if busSer[a].find('strong').text=='Recreation':\n",
    "                    l3=busSer[a].find('ul', class_='checkList row').text\n",
    "                    data['Recreation']=l3.split()\n",
    "                if busSer[a].find('strong').text=='Travel':\n",
    "                    l4=busSer[a].find('ul', class_='checkList row').text\n",
    "                    data['Travel']=l4.split()\n",
    "                if busSer[a].find('strong').text=='Personal Services':\n",
    "                    l5=busSer[a].find('ul', class_='checkList row').text\n",
    "                    data['Personal Services']=l5.split()\n",
    "                    \n",
    "                with open(\"/Users/tanujmathur/data_file.json\", \"a+\") as write_file:\n",
    "                    json.dump(data, write_file)\n",
    "            \n",
    "        except IndexError:\n",
    "            floors=None\n",
    "            data['floors']=floors   \n",
    "        except:\n",
    "            print(\"anomolous Data encountered\")\n",
    "    \n",
    "        \n",
    "        \n",
    "    url='https://me.cleartrip.com/hotels/united-states/miami?page='+str(i+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
